\documentclass[11pt,twoside,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{url} % typeset URL's reasonably
\usepackage{listings}

\usepackage{pslatex} % Use Postscript fonts

\usepackage{subcaption}
\usepackage{color}
\usepackage{multirow}
\usepackage{makecell}

\usepackage{mathptmx}
\usepackage{amsmath}

%define some own functions
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}} 
\def\D{\mathrm{d}}


\begin{document}

\title{Training Spiking Restricted~Boltzmann~Machine by Minimising Contrastive~Divergence}
\author{
Qian~Liu
\thanks{
The author is with the School of Computer Science, University of Manchester, Manchester M13 9PL, U.K. 
(e-mail:qian.liu-3@manchester.ac.uk}
}
\maketitle
\thispagestyle{empty}

\begin{abstract}
In order to implement training of Spiking Deep~Belief~Networks~(SDBNs) on SpiNNaker, this paper studies the layer-by-layer training of spiking Restricted~Boltzmann~Machine~(RBM) of a DBN.
The study starts from understanding the original problem, Products of Experts~(PoE), which was solved by using Contrastive~Divergence~(CD).
It involves utilising Markov~Chain~Monte~Carlo~(MCMC) sampling to present the distribution of a certain untraceable high-dimensional probability model function, e.g. PoE.
Among these sampling algorithms, Gibbs method is introduced and used in PoE problem.
Instead of minimising the original objective function of Kullback-Leibler divergence, the contrastive divergence is exploited to solve PoE.
Then the study continues on applying CD to RBM.
Finally, on-line learning methods only spiking neurons used are explored to train spiking RBM.

\end{abstract}
\section{Why CD?\cite{hinton2002training,woodfordnotes}}
The probability of a data point, $ \mathbf{x} $,  is modelled by the function $f(\mathbf{x} \mid \mathbf{\theta} )$, given the model parameters $ \mathbf{\theta} $. 
Thus, given a set of data points $ \mathbf{X} $, the probability of each data point,  $ \mathbf{x} $, is normalised by a partition function $Z( \mathbf{\theta})$ :
\begin{equation}
p(\mathbf{x} \mid \mathbf{\theta} ) = \dfrac{f(\mathbf{x} \mid \mathbf{\theta} )}{Z( \mathbf{\theta})}
\end{equation}
where the partition function is defined as:
\begin{equation}
Z( \mathbf{\theta}) = \int f(\mathbf{x} \mid \mathbf{\theta} )\D\mathbf{x}, \text{when  $ \mathbf{X} $ is continuous, or}
\end{equation}

\begin{equation}
Z( \mathbf{\theta}) = \sum_\mathbf{X} f(\mathbf{x} \mid \mathbf{\theta} ), \text{when  $ \mathbf{X} $ is discrete.}
\end{equation}
The purpose of learning is to tune the model parameter $ \mathbf{\theta} $ to fit the data $ \mathbf{X}  $. 
The objective function is to maximise the probability product:
\begin{equation}
 p(\mathbf{X} \mid \mathbf{\theta} ) = \prod_\mathbf{X} p(\mathbf{x} \mid \mathbf{\theta} ) =  \prod_\mathbf{X}\dfrac{f(\mathbf{x} \mid \mathbf{\theta} )}{Z( \mathbf{\theta})}.
\end{equation}
 This equals to maximise the log of the probability product, also known as the negative energy function:
\begin{equation}
  \log p(\mathbf{X} \mid \mathbf{\theta} ) = -E(\mathbf{X} \mid \mathbf{\theta} )  =  \sum_\mathbf{X}[\log f(\mathbf{x} \mid \mathbf{\theta} ) - \log Z( \mathbf{\theta})] 
%  = K(<\log f(\mathbf{x} \mid \mathbf{\theta} )>_{data} - \log Z( \mathbf{\theta}))
\end{equation}

Imagine three different conditions (probability function).
First, $f(\mathbf{x} \mid \mathbf{\theta} )$ is the probability density function (pdf) of a normal distribution $\mathcal{N}(x \mid \mu, \sigma )$.
Data vector $ \mathbf{x} $ is just a one dimensional data point, $x$.
$ Z( \mathbf{\theta}) $ equals to 1, thus $p(x \mid \mathbf{\theta} ) = \mathcal{N}(x \mid \mu, \sigma )$.
\begin{equation}
\begin{split}
  &\log p(X \mid \theta ) =  \sum_X \log [\frac{1}{\sigma \sqrt{2\pi}} \exp(-\frac{(x-\mu)^{2}}{2\sigma^{2}})] \\
  &\dfrac{\partial  \log p(X \mid \theta ) }{\partial \mu}= \sum_X -\frac{1}{2\sigma^{2}}\dfrac{\partial (\mu-x)^{2}}{\partial \mu} = \sum_X -\frac{1}{\sigma^{2}}(\mu-x)\\
\end{split}
\end{equation}
\subsection{PoE Problem}
\subsection{MCMC Sampling}
\subsection{CD Instead of KL}
\section{RBM\cite{zhang2013rbm}}
\subsection{Objective Function}
\subsection{CD with 1-step Reconstruction}
\section{Spiking RBM\cite{neftci2013event}}

\bibliography{ref} 
\bibliographystyle{ieeetr}
\end{document}