\documentclass[11pt,twoside,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{url} % typeset URL's reasonably
\usepackage{listings}

\usepackage{pslatex} % Use Postscript fonts

\usepackage{subcaption}
\usepackage{color}
\usepackage{multirow}
\usepackage{makecell}

\usepackage{mathptmx}
\usepackage{amsmath}

%define some own functions
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}} 
\def\D{\mathrm{d}}


\begin{document}

\title{Training Spiking Restricted~Boltzmann~Machine by Minimising Contrastive~Divergence}
\author{
Qian~Liu
\thanks{
The author is with the School of Computer Science, University of Manchester, Manchester M13 9PL, U.K. 
(e-mail:qian.liu-3@manchester.ac.uk}
}
\maketitle
\thispagestyle{empty}

\begin{abstract}
In order to implement training of Spiking Deep~Belief~Networks~(SDBNs) on SpiNNaker, this paper studies the layer-by-layer training of spiking Restricted~Boltzmann~Machine~(RBM) of a DBN.
The study starts from understanding the original problem, Products of Experts~(PoE), which was solved by using Contrastive~Divergence~(CD).
It involves utilising Markov~Chain~Monte~Carlo~(MCMC) sampling to present the distribution of a certain untraceable high-dimensional probability model function, e.g. PoE.
Among these sampling algorithms, Gibbs method is introduced and used in PoE problem.
Instead of minimising the original objective function of Kullback-Leibler divergence, the contrastive divergence is exploited to solve PoE.
Then the study continues on applying CD to RBM.
Finally, on-line learning methods only spiking neurons used are explored to train spiking RBM.

\end{abstract}
\section{Why CD?\cite{hinton2002training,woodfordnotes}}
The probability of a data point, $ \mathbf{x} $,  is modelled by the function $f(\mathbf{x} \mid \mathbf{\theta} )$, given the model parameters $ \mathbf{\theta} $. 
Thus, given a set of data points $ \mathbf{X}=\{\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_k, \mathbf{x}_{k+1}, ..., \mathbf{x}_K\} $, the probability of each data point,  $ \mathbf{x} $, is normalised by a partition function $Z( \mathbf{\theta})$ :
\begin{equation}
p(\mathbf{x} \mid \mathbf{\theta} ) = \dfrac{f(\mathbf{x} \mid \mathbf{\theta} )}{Z( \mathbf{\theta})}
\end{equation}
where the partition function is defined as:
\begin{equation}
Z( \mathbf{\theta}) = \int f(\mathbf{x} \mid \mathbf{\theta} )\D\mathbf{x}, \text{when  $ \mathbf{X} $ is continuous, or}
\end{equation}

\begin{equation}
Z( \mathbf{\theta}) = \sum_{k=1}^K f(\mathbf{x}_k \mid \mathbf{\theta} ), \text{when  $ \mathbf{X} $ is discrete.}
\end{equation}
The purpose of learning is to tune the model parameter $ \mathbf{\theta} $ to fit the data $ \mathbf{X}  $. 
The objective function is to maximise the probability product:
\begin{equation}
 p(\mathbf{X} \mid \mathbf{\theta} ) = \prod_{k=1}^K p(\mathbf{x}_k \mid \mathbf{\theta} ) =  \prod_{k=1}^K\dfrac{f(\mathbf{x}_k \mid \mathbf{\theta} )}{Z( \mathbf{\theta})}.
\end{equation}
 This equals to maximise the log of the probability product, also known as the negative energy function:
\begin{equation}
  \log p(\mathbf{X} \mid \mathbf{\theta} ) = -E(\mathbf{X} \mid \mathbf{\theta} )  =  \sum_{k=1}^K[\log f(\mathbf{x}_k \mid \mathbf{\theta} ) - \log Z( \mathbf{\theta})] 
%  = K(<\log f(\mathbf{x} \mid \mathbf{\theta} )>_{data} - \log Z( \mathbf{\theta}))
\end{equation}

Imagine three different conditions (probability function) as following.

\textbf{First}, $f(\mathbf{x} \mid \mathbf{\theta} )$ is the probability density function (pdf) of a normal distribution $\mathcal{N}(x \mid \mu, \sigma )$.
Data vector $ \mathbf{x} $ is just a one dimensional data point, $x$.
$ Z( \mathbf{\theta}) $ equals to 1, thus $p(x \mid \mathbf{\theta} ) = \mathcal{N}(x \mid \mu, \sigma )$.
\begin{equation}
\log p(X \mid \theta ) =  \sum_{k=1}^K \log [\frac{1}{\sigma \sqrt{2\pi}} \exp(-\frac{(x_k-\mu)^{2}}{2\sigma^{2}})]
\label{pdf}
\end{equation}
To maximise Equation~\ref{pdf} is to find the Maximum Likelihood Estimation (MLE) of parameters $ \mu $ and $ \sigma $, by deriving from the partial differential equations when they are equal to 0. 
\begin{equation}
\left\{
\begin{aligned}
   &\dfrac{\partial  \log p(X \mid \theta ) }{\partial \mu}= \sum_{k=1}^K -\frac{1}{2\sigma^{2}}\dfrac{\partial (\mu-x_k)^{2}}{\partial \mu} = \sum_{k=1}^K -\frac{1}{\sigma^{2}}(\mu-x_k) = 0 \quad\\
   &\dfrac{\partial  \log p(X \mid \theta ) }{\partial \sigma^{2}}= -\frac{K}{2\sigma^{2}}+\frac{1}{2\sigma^{4}}\sum_{k=1}^K x_k^{2} -\frac{\mu}{\sigma^{4}}\sum_{k=1}^K x_k + \frac{K\mu^{2}}{2\sigma^{4}} = 0 \quad\\
\end{aligned}
\right.
\end{equation}
\begin{equation}
\left\{
\begin{aligned}
    &\mu= \frac{1}{K}\sum_{k=1}^K x_k  \quad\\
    &\sigma^{2} = \frac{1}{K}\sum_{k=1}^K x_k^{2} - (\frac{1}{K}\sum_{k=1}^K x_k)^{2}
\end{aligned}
\right.
\end{equation}
$p(x\mid \mathbf{\theta} )$ here is the function of two dimensional parameters $\mu$ and $\theta$, and searching the highest point in the parameter space ``is equivalent to being in the field on a clear, sunny day,''~\cite{woodfordnotes} seeing the point straight away.

\textbf{Second}, the probability model function changes to be the sum of N normal distributions: 
\begin{equation}
f(x \mid \mathbf{\theta} ) = \sum_{i=1}^N\mathcal{N}(x \mid \mu_i, \sigma_i )
\end{equation}
\subsection{PoE Problem}
\subsection{MCMC Sampling}
\subsection{CD Instead of KL}
\section{RBM\cite{zhang2013rbm}}
\subsection{Objective Function}
\subsection{CD with 1-step Reconstruction}
\section{Spiking RBM\cite{neftci2013event}}

\bibliography{ref} 
\bibliographystyle{ieeetr}
\end{document}